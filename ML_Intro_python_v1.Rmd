---
title: "Machine Learning Intro in Python"
author: "Ricardo A. DeMoya"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reticulate)
if (!dir.exists("./python-env-ML_intro/")) {
  reticulate::conda_create("./python-env-ML_intro/")
}
#should default to virtual environment “r-reticulate”

# search for a Python installation in your working directory
reticulate::conda_install(
    packages = c("scikit-learn", "pandas", "matplotlib","scipy"),
    envname = "./python-env-ML_intro/"
)
```

# A quick introduction to machine learning
Most fo the world is familiar with the phrase "machine learning". So here we hope to fill in some knowledge gaps and misconceptions as well as introduce you to a great coding language with applicable use cases. 
```{r,include=FALSE}
reticulate::conda_list()# Lists the environments
py_install("matplotlib")
py_install("scipy")
```


```{r,include=FALSE}
py_config()
```


```{python}
import sklearn as sk
import numpy as np
import pandas as pd
import matplotlib as plt
```

```{python}
np.arange(1, 10)
df = pd.DataFrame(data = {"sequence":np.arange(1,20,.01)})
df
```

```{python}
from sklearn.cluster import AffinityPropagation
from sklearn.datasets import make_blobs
# #############################################################################
# Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,
                            random_state=0)
# Compute Affinity Propagation
af = AffinityPropagation(preference=-50).fit(X)
cluster_centers_indices = af.cluster_centers_indices_
labels = af.labels_
n_clusters_ = len(cluster_centers_indices)
# #############################################################################
# Plot result
import matplotlib.pyplot as plt
from itertools import cycle
plt.close('all')
plt.figure(1)
plt.clf()
colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
    class_members = labels == k
    cluster_center = X[cluster_centers_indices[k]]
    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
             markeredgecolor='k', markersize=14)
    for x in X[class_members]:
        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)
plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
```

```{python}
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=0)
X = [[ 1,  2,  3],  # 2 samples, 3 features
     [11, 12, 13]]
y = [0, 1]  # classes of each sample
clf.fit(X, y)
clf.predict(X)
```

```{python}
from sklearn.datasets import load_diabetes
from sklearn.ensemble import (
    GradientBoostingRegressor,
    RandomForestRegressor,
    VotingRegressor,
)
from sklearn.linear_model import LinearRegression
```

```{python}
X, y = load_diabetes(return_X_y=True)

# Train classifiers
reg1 = GradientBoostingRegressor(random_state=1)
reg2 = RandomForestRegressor(random_state=1)
reg3 = LinearRegression()

reg1.fit(X, y)
reg2.fit(X, y)
reg3.fit(X, y)

ereg = VotingRegressor([("gb", reg1), ("rf", reg2), ("lr", reg3)])
ereg.fit(X, y)
```
```{python}
xt = X[:20]

pred1 = reg1.predict(xt)
pred2 = reg2.predict(xt)
pred3 = reg3.predict(xt)
pred4 = ereg.predict(xt)
```

### Testing out linear regression in scikit-learn
Here we load the diabetes data set and set the as_frame parameter to TRUE so we get a nice neat data frame to work with. We can see the first five rows and the columns names.
```{python}
from sklearn.datasets import load_diabetes
# Load the dataset
diabetes_data = sk.datasets.load_diabetes(as_frame=True)
pd.DataFrame.head(diabetes_data.data)
```
Next, we checked the feature names which are the same as the column names from before
```{python}
diabetes_data.feature_names # The names of types of data
```
### Let's do a linear regression
Here we use the diabetes data set to perform a lilnear regression. 
```{python}
from sklearn.linear_model import LinearRegression
print(type(diabetes_data['data']))
print(list(diabetes_data))
```
#### Let's get a description of the dataset
We can get a detailed description of the data sets from sklearn. This feature is really great, but remember data is not always so clear or described in such detail.
```{python}
# Description of the data set
print(diabetes_data['DESCR'])
```
### What does the data look like
Here we can look at all the data and see what it looks like
```{python}
print(diabetes_data['data'])
```

Next we can look at what dimensions the data has using the shape attribute.
```{python}
diabetes_data.data.shape
```
### Linear Regression using the diabetes data set
Checking out the data and we can see 10 columns.
```{python}
diabetes_data.data.sample(5)
```

```{python}
df = diabetes_data.frame.assign(Progression=diabetes_data.target)
df
```

#### Check out the numbers
Check for missing values in the data sets.
```{python}
df.isna().sum()
```
No missing values!

Let's get a description of the data frame.
```{python}
df.describe()
```
Next we can get information on the data set.
```{python}
df.info()
```
### The Linear correlation between variables
```{python}
corr = df.corr()
corr
```


```{python}
diabetes_data.data
```


### Let's plot the data
```{python}
import matplotlib.pyplot as plt
# Don't plot the sex data
features = diabetes_data['feature_names']
features.remove('sex')

# Plot
fig, axs = plt.subplots(3, 3)
fig.suptitle('Diabetes Dataset')
for i in range(3):
    for j in range(3):
        n = j + i * 3
        feature = features[n]
        axs[i, j].scatter(diabetes_data['data'][feature], diabetes_data['target'], s=1)
        axs[i, j].set_xlabel(feature)
        axs[i, j].set_ylabel('target')
plt.tight_layout()
plt.show()
```

